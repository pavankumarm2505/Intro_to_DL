{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment - 0010\n",
        "1. Extend your Neural Network to work with various optimizer and design various regularizer to avoid overfitting.\n",
        "2. Test your model on same set of datasets provided in your prior assignment such as MNIST and F-MNIST/K-MNIST\n",
        "2. No - Plagarism. If solutions match, then the student who shared the solution and students who copied will be reported to USF and will get F in this course.\n",
        "3. You can discuss, but have to code and provide your own solution. Provide name of the person or persons with whom you had discussion"
      ],
      "metadata": {
        "id": "oC1UJhcn6cun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Things to Do\n",
        " \n",
        "1. Now you will Create a 4 layer Neural Network, (W1, W2, W3, W4)\n",
        "2. H1 = $X \\cdot W1$ + b1, Z1 = $\\sigma$(H1), where $\\sigma$ is your activation function, $\\cdot$ is your hardamard product, H denotes pre-activation and Z denotes post-activation.\n",
        "3. Recursively construct your NN, H2 = (Z1 $\\cdot$W2) + b2, Z2 = $\\sigma$(H2), H3 = (Z2 $\\cdot$ W3) + b3, z3 = $σ$(H3) and final output layer is H4 = (Z3 $\\cdot$ W4) + b4, z3 = $σ^1$(H4), where $σ^1$ is your softmax function.\n",
        "4. The objective function that you have used in your prior assignment is termed as cross-entropy, denoted as follows J($\\theta$) = $-{(y\\log(p) + (1 - y)\\log(1 - p))}$ for binary classification, whereas for multi-class classification we denote it as J($\\theta$) = $-\\sum_{c=1}^My_{o,c}\\log(p_{o,c})$, where M - number of classes, log - the natural log, y - binary indicator (0 or 1) if class label c is the correct classification for observation o and finally p - predicted probability observation o is of class c.\n",
        "5. Now in this assignment you will be using 2 regularizer L1 and L2 and will modify above equation as follows: For L1 J($θ$)_1 = J($\\theta$) + $λ$$||w||^1_1$, where $λ$ is another set of hyper-parameter that adjusts your L1 penality,   $||w||^1_1$ = $\\sum_{i=1}^N$ $|w_i|$ adds absolute value of magnitude as penality term to the loss function, $N$ denotes total number of layers.\n",
        "6. Whereas L2 is defined as follows: J($θ$)_2 = J($\\theta$) + $λ$$||w||^2_2$, where  $||w||^2_2$ = $\\sum_{i=1}^N$ $|w_i^2|$ adds squared magnitude as penality term to the loss function.\n",
        "6. Play with various optimizer such as Adam, RMSProp, Momentum\n",
        "4. Normalize your data within 0-1 range, create one-hot encoding for your labels.\n",
        "5. Take 3digit of your first name, and convert them based on character indices - For example: Ank ---> 1 14 11, so my seed is 11411. This will ensure each students have unique seeds.\n",
        "6. Run your experiments for minimum $20$ epochs. Remember one epoch is entire pass through data.\n",
        "7. You will run each experiment min $7$ times and report your average accuracy and standard error -- Change seed per trial\n",
        "8. We have provided 2 folders, members of Teams A - E will work on data that says Team_red and Team F- K will work on data that says Team_blue\n",
        "9. Important: All parameter optimization should be done on Validation/Dev set, you will only do one pass inference over test set.\n",
        "10. Complete all code blocks highlighted as TODO\n",
        "11. Sumbit your code, your report with findings and upload on canvas."
      ],
      "metadata": {
        "id": "tRw25rAH7Owu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rubric\n",
        "1. Total marks = 10% of your total grade. \n",
        "2. This assignment is of worth 100 points, which will be converted into 10.\n",
        "3. If you train on Vaidation and test sets ---- (0 or -100 Points)\n",
        "4. Results with only one trial is reported ---- (-20 points)\n",
        "5. Experiments are performed on only one dataset ---- (-50 points)\n",
        "6. Code is not working or has some bugs, Depending on bug ---- (-15 to -60 points)\n",
        "7. Only L1 regularization is used ---- (-50 points)\n",
        "8. This assignment requires you to have 4 layer model, if only 3 layer is shown --- (-25 points)\n",
        "9. No test or comparsion with optimizers, Atleast show one experiments that provide reasoning why X optimizer is selected  --- (-20 points)\n",
        "9. Submiting some random solution without following template ---- (-100 points)\n"
      ],
      "metadata": {
        "id": "OQPpqCXdHPSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "seed = 16122+30\n",
        "np.random.seed(seed) ## Use your own unique seed\n",
        "tf.random.set_seed(seed) ## Use your own unique seed\n",
        "\n",
        "## Author - Ankur Mali\n",
        "## Code designed for intro to DL at University of South Florida"
      ],
      "metadata": {
        "id": "EJJE2RGZL8qo"
      },
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define model specific parameters\n",
        "## Repalce None with values\n",
        "size_input = 28*28\n",
        "size_hidden = 128\n",
        "size_hidden1 = 64\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 64\n",
        "size_output = 10\n",
        "number_of_train_examples = 50000\n",
        "number_of_dev_examples = 10000\n",
        "number_of_test_examples = 10000\n",
        "batch_size = 1024\n",
        "NUM_EPOCHS = 20"
      ],
      "metadata": {
        "id": "D32LRkMJGV2V"
      },
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO\n",
        "## Load Data\n",
        "X_train = np.load('x_train.npy').reshape(-1,784)\n",
        "X_test = np.load('x_test.npy').reshape(-1,784)\n",
        "# with np.load('train-imgs.npz') as data:\n",
        "#   arrays = data.files\n",
        "#   arr = data[arrays [0]]\n",
        "# new_shape = (-1,784)\n",
        "# X_train = np.reshape(arr, new_shape)\n",
        "\n",
        "# with np.load('test-imgs.npz') as data:\n",
        "#   arrays = data.files\n",
        "#   arr = data[arrays [0]]\n",
        "# new_shape = (-1,784)\n",
        "# X_test = np.reshape(arr, new_shape)\n",
        "# # X_test = np.load('X_test.npy').reshape(-1,784)\n",
        "# with np.load('train-labels.npz') as data:\n",
        "#   arrays = data.files\n",
        "#   Y_train = data[arrays [0]]\n",
        "\n",
        "# with np.load('test-labels.npz') as data:\n",
        "#   arrays = data.files\n",
        "#   Y_test = data[arrays [0]]\n",
        "\n",
        "\n",
        "## Create Validation data from train splits\n",
        "\n",
        "Y_train = np.load('y_train.npy')\n",
        "Y_test = np.load('y_test.npy')\n",
        "# Y_train = np.load('train-labels.npz')\n",
        "# Y_test = np.load('test-labels.npz')\n",
        "# X_train = X_train.reshape(50000, 28*28)\n",
        "# X_val = X_val.reshape(10000, 28*28)\n",
        "# X_test = X_test.reshape(10000, 28*28)\n",
        "X_val, Y_val = X_train[50000:], Y_train[50000:] \n",
        "X_train,Y_train = X_train[:50000], Y_train[:50000]\n",
        "\n"
      ],
      "metadata": {
        "id": "3IVzVDIR6arH"
      },
      "execution_count": 409,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "## TODO\n",
        "# Normalize your X\n",
        "# X_train = (X_train - np.mean(X_train,axis=0)) / np.std(X_train,axis=0)\n",
        "# X_val = (X_val - np.mean(X_val,axis=0)) / np.std(X_val,axis=0)\n",
        "# X_test = (X_test - np.mean(X_test,axis=0)) / np.std(X_test,axis=0)\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255\n",
        "# Convert your labels into one-hot encoding\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, size_output)\n",
        "Y_val = tf.keras.utils.to_categorical(Y_val,size_output)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test,size_output)\n",
        "# Hint look into function tf.keras.utils.to_categorical or Other function is tf.one_hot()"
      ],
      "metadata": {
        "id": "qSrIHwVxRbYc"
      },
      "execution_count": 410,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparam_l1 = 0.01 # Set the strength of L1 regularization penalty\n",
        "hyperparam_l2 = 0.01"
      ],
      "metadata": {
        "id": "prONHHWm6C4T"
      },
      "execution_count": 411,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output,device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    ### TODO\n",
        "    ### Declare all your variables\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3]))\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output]))\n",
        "\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    \n",
        "    \n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    ## TODO\n",
        "    \n",
        "    # Convert your pred and true to tf.float32\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # Ensure your shapes are (batch_size, size_output)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    # This above function will add softmax to your final layer and then perform cross-entropy\n",
        "    \n",
        "\n",
        "    # TODO\n",
        "    # Modify above loss and add L1 or L2 regularization\n",
        "    l1_reg = 0.0001 * tf.reduce_sum(tf.abs(self.variables[0]))+0.0001 * tf.reduce_sum(tf.abs(self.variables[1]))+0.0001 * tf.reduce_sum(tf.abs(self.variables[2]))+0.0001 * tf.reduce_sum(tf.abs(self.variables[3]))+0.0001 * tf.reduce_sum(tf.abs(self.variables[4]))+0.0001 * tf.reduce_sum(tf.abs(self.variables[5]))+0.0001 * tf.reduce_sum(tf.abs(self.variables[6]))+0.0001 * tf.reduce_sum(tf.abs(self.variables[7]))\n",
        "    # l2_reg = 0.0001 * tf.reduce_sum(tf.square(self.variables[0]))+0.0001 * tf.reduce_sum(tf.square(self.variables[1]))+0.0001 * tf.reduce_sum(tf.square(self.variables[2]))+0.0001 * tf.reduce_sum(tf.square(self.variables[3]))+0.0001 * tf.reduce_sum(tf.square(self.variables[4]))+0.0001 * tf.reduce_sum(tf.square(self.variables[5]))+0.0001 * tf.reduce_sum(tf.square(self.variables[6]))+0.0001 * tf.reduce_sum(tf.square(self.variables[7]))\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)+l1_reg\n",
        "    \n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "\n",
        "    lr = 0.003 #Play with your learning rate\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = lr) # Play with various Optimizers and pick the one that works best for your design\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables)) # Optimizer\n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    ## TODO\n",
        "    H1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    Z1 = tf.nn.sigmoid(H1)\n",
        "    H2 = tf.matmul(Z1, self.W2) + self.b2\n",
        "    Z2 = tf.nn.sigmoid(H2)\n",
        "    H3 = tf.matmul(Z2, self.W3) + self.b3\n",
        "    Z3 = tf.nn.sigmoid(H3)\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    \n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(Z3, self.W4) + self.b4 # Logits\n",
        "    \n",
        "    # Remember the loss function has keras loss objective, that explicitly applies softmax over logits\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "ycucHIu9OWWi"
      },
      "execution_count": 412,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(25, seed=epoch*(seed)).batch(batch_size)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_gpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  \n",
        "  ## TODO\n",
        "\n",
        "  ## Now calculate Validation Accuracy\n",
        "  preds = mlp_on_gpu.forward(X_val)\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_val, 1))\n",
        "  accuracy_z =  tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy_z.numpy()\n",
        "  \n",
        "  \n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "id": "vDOs-cNLQbPw",
        "outputId": "97794092-a00c-42e3-b4e3-ded9c9f3f349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.6661\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.006334930419921875 \n",
            "\n",
            "Validation Accuracy: 0.6787\n",
            "\n",
            "Train Accuracy: 0.7964\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.004853572692871093 \n",
            "\n",
            "Validation Accuracy: 0.8096\n",
            "\n",
            "Train Accuracy: 0.8472\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.004238345642089844 \n",
            "\n",
            "Validation Accuracy: 0.8563\n",
            "\n",
            "Train Accuracy: 0.8743\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0037878671264648437 \n",
            "\n",
            "Validation Accuracy: 0.8757\n",
            "\n",
            "Train Accuracy: 0.8918\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0034184320068359375 \n",
            "\n",
            "Validation Accuracy: 0.8918\n",
            "\n",
            "Train Accuracy: 0.9026\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.00310399169921875 \n",
            "\n",
            "Validation Accuracy: 0.9047\n",
            "\n",
            "Train Accuracy: 0.9122\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0028339202880859377 \n",
            "\n",
            "Validation Accuracy: 0.9118\n",
            "\n",
            "Train Accuracy: 0.9196\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.002602145690917969 \n",
            "\n",
            "Validation Accuracy: 0.9175\n",
            "\n",
            "Train Accuracy: 0.9259\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.002401917724609375 \n",
            "\n",
            "Validation Accuracy: 0.9215\n",
            "\n",
            "Train Accuracy: 0.9303\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0022292594909667967 \n",
            "\n",
            "Validation Accuracy: 0.9255\n",
            "\n",
            "Train Accuracy: 0.9340\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.002080886993408203 \n",
            "\n",
            "Validation Accuracy: 0.9297\n",
            "\n",
            "Train Accuracy: 0.9374\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0019534791564941407 \n",
            "\n",
            "Validation Accuracy: 0.9324\n",
            "\n",
            "Train Accuracy: 0.9397\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0018430178833007812 \n",
            "\n",
            "Validation Accuracy: 0.9333\n",
            "\n",
            "Train Accuracy: 0.9418\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.001747191619873047 \n",
            "\n",
            "Validation Accuracy: 0.9371\n",
            "\n",
            "Train Accuracy: 0.9438\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0016624397277832032 \n",
            "\n",
            "Validation Accuracy: 0.9377\n",
            "\n",
            "Train Accuracy: 0.9460\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0015875656127929687 \n",
            "\n",
            "Validation Accuracy: 0.9392\n",
            "\n",
            "Train Accuracy: 0.9477\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0015207272338867187 \n",
            "\n",
            "Validation Accuracy: 0.9408\n",
            "\n",
            "Train Accuracy: 0.9488\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.001460928497314453 \n",
            "\n",
            "Validation Accuracy: 0.9413\n",
            "\n",
            "Train Accuracy: 0.9505\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0014072869873046874 \n",
            "\n",
            "Validation Accuracy: 0.9426\n",
            "\n",
            "Train Accuracy: 0.9518\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0013582444763183593 \n",
            "\n",
            "Validation Accuracy: 0.9436\n",
            "\n",
            "Total time taken (in seconds): 151.82\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(25, seed=epoch*(seed)).batch(batch_size)\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_gpu.forward(inputs)\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "accuracy_test=0.0\n",
        "cur_test_acc=0.0\n",
        "avg_mse = np.sum(test_loss_total.numpy()) / X_test.shape[0]\n",
        "print('Test Standard error: {:.4f}'.format(np.sqrt(avg_mse)))\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "preds = tf.nn.softmax(preds)\n",
        "correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_test, 1))\n",
        "accuracy_test =   tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy_test.numpy()\n",
        "\n",
        "print('\\nTest Accuracy: {:.4f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "id": "jpZC6lpCRSe6",
        "outputId": "4dcbe180-f79f-4ce4-8641-d39060d119c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 414,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Standard error: 0.0523\n",
            "\n",
            "Test Accuracy: 0.9365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_9gDm9rm5hm"
      },
      "execution_count": 406,
      "outputs": []
    }
  ]
}