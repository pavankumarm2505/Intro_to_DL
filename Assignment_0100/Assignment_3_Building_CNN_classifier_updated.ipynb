{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavankumarm2505/Intro_to_DL/blob/main/Assignment_0100/Assignment_3_Building_CNN_classifier_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this tutorial we will build basic CNN for image classification and then perform few test to understand overall working on CNN.\n",
        "Author :- Dr. Ankur Mali -- (Intro to DL - USF)\n",
        "* We will define our model and learn how to use keras module to build custom layers\n",
        "* We will also design our own training loop, that is identical to model.fit in Keras.\n",
        "* The aim of this excercise is to teach, how to use exisiting Tensorflow API to construct our own module and integrate it with tf.keras API."
      ],
      "metadata": {
        "id": "iEL0LqOvNctE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SK3DMbzThNBc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(16145)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Things to do\n",
        "* Remember to Normalize your data and create validation split from train set.\n",
        "* Learn about tf.data, tf.slices and also tf.records"
      ],
      "metadata": {
        "id": "j3wnZ5tvOTCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_val = x_train[45000:50000]\n",
        "y_val = y_train[45000:50000]\n",
        "x_train = x_train[0:45000]\n",
        "y_train = y_train[0:45000]\n",
        "x_train = x_train.astype(np.float32).reshape(-1,32,32,3) / 255.0\n",
        "x_val = x_val.astype(np.float32).reshape(-1,32,32,3) / 255.0\n",
        "x_test = x_test.astype(np.float32).reshape(-1,32,32,3) / 255.0\n",
        "y_train = tf.one_hot(y_train, depth=10)\n",
        "y_val = tf.one_hot(y_val, depth=10)\n",
        "y_test = tf.one_hot(y_test, depth=10)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(x_val.shape)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128)\n",
        "train_dataset_full = train_dataset.shuffle(buffer_size=1024).batch(len(train_dataset))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(128)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(128)\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWo3ho3whTWU",
        "outputId": "88c98761-6237-4b6c-dee4-09e401ad0d7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 5s 0us/step\n",
            "(45000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(5000, 32, 32, 3)\n",
            "352\n",
            "79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create your custom CNN class\n",
        "* Convolution layers has 4D weights of size (h,w,input_feature, output_feature), where h=height of your kernel and w = width of our kernel. If you add batches then it is 5D.\n",
        "* Now your model will convolve across your input feature map with kernel and create output feature map, that is then passed to next layer.\n",
        "* As we have learned in our prior class, to initialize your weights, we use tf.Variable(weight_init(size)), tf.keras.layers.Conv2D will do this for you. Play with the function and see how it works for your problem.\n",
        "* Few important concepts, learn to save your model after every k epochs and start re-training from last checkpoint. This is very useful, and you don't need to retrain your model from scratch.\n"
      ],
      "metadata": {
        "id": "Rs9r9QDvO48Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageRecognitionCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, num_classes, device='cpu:0', checkpoint_directory=None, checkpoint_prefix = None):\n",
        "        ''' Define the parameterized layers used during forward-pass, the device\n",
        "            where you would like to run the computation (GPU, TPU, CPU) on and the checkpoint\n",
        "            directory.\n",
        "            \n",
        "            Args:\n",
        "                num_classes: the number of labels in the network.\n",
        "                device: string, 'cpu:n' or 'gpu:n' (n can vary). Default, 'cpu:0'.\n",
        "                checkpoint_directory: the directory where you would like to save or \n",
        "                                      restore a model.\n",
        "        ''' \n",
        "        super(ImageRecognitionCNN, self).__init__()\n",
        "        \n",
        "        # Initialize layers\n",
        "        self.conv1 = tf.keras.layers.Conv2D(256, 3, padding='same', activation=None)\n",
        "        self.conv2 = tf.keras.layers.Conv2D(256, 3,padding='same', activation=None)\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D()\n",
        "        self.conv3 = tf.keras.layers.Conv2D(128, 3, padding='same', activation=None)\n",
        "        self.conv4 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
        "        self.conv5 = tf.keras.layers.Conv2D(32, 3, padding='same', activation=None)\n",
        "        self.conv8 = tf.keras.layers.Conv2D(num_classes, 1, padding='same', activation=None)\n",
        "        \n",
        "        # Define the device \n",
        "        self.device = device\n",
        "        \n",
        "        # Define the checkpoint directory\n",
        "        self.checkpoint_directory = checkpoint_directory\n",
        "        self.checkpoint_prefix = checkpoint_prefix\n",
        "        self.acc = tf.keras.metrics.Accuracy()\n",
        "\n",
        "    #@tf.function\n",
        "    def predict(self, images, training):\n",
        "        \"\"\" Predicts the probability of each class, based on the input sample.\n",
        "            \n",
        "            Args:\n",
        "                images: 4D tensor. Either an image or a batch of images.\n",
        "                training: Boolean. Either the network is predicting in\n",
        "                          training mode or not.\n",
        "        \"\"\"\n",
        "        x = self.conv1(images)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv3(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv4(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv5(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv8(x)\n",
        "        x = self.pool1(x)\n",
        "        x = tf.reshape(x, (-1, 1, 10))\n",
        "        return x\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def loss_fn(self, images, target, training):\n",
        "        \"\"\" Defines the loss function used during \n",
        "            training.         \n",
        "        \"\"\"\n",
        "        preds = self.predict(images, training)\n",
        "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=preds)\n",
        "        return loss\n",
        "\n",
        "    @tf.function\n",
        "    def grads_fn(self, images, target, training):\n",
        "        \"\"\" Dynamically computes the gradients of the loss value\n",
        "            with respect to the parameters of the model, in each\n",
        "            forward pass.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.loss_fn(images, target, training)\n",
        "        return tape.gradient(loss, self.variables)\n",
        "\n",
        "    \n",
        "    @tf.function\n",
        "    def restore_model(self):\n",
        "        \"\"\" Function to restore trained model.\n",
        "        \"\"\"\n",
        "        with tf.device(self.device):\n",
        "            # Run the model once to initialize variables\n",
        "            dummy_input = tf.constant(tf.zeros((1,48,48,1)))\n",
        "            dummy_pred = self.predict(dummy_input, training=False)\n",
        "            # Restore the variables of the model\n",
        "            saver = tf.compat.v1.train.Saver(self.variables)\n",
        "            saver.restore(tf.train.latest_checkpoint\n",
        "                          (self.checkpoint_directory))\n",
        "    @tf.function\n",
        "    def save_model(self, global_step=0):\n",
        "        \"\"\" Function to save trained model.\n",
        "        \"\"\"\n",
        "        \n",
        "        # tf.compat.v1.train.Saver(self.variables).save(save_path=self.checkpoint_directory, \n",
        "        #                                global_step=global_step)   \n",
        "        return None\n",
        "    \n",
        "    @tf.function\n",
        "    def compute_accuracy_2(self, images, targets):\n",
        "        \"\"\" Compute the accuracy on the input data.\n",
        "        \"\"\"\n",
        "        with tf.device(self.device):\n",
        "            \n",
        "            # Predict the probability of each class\n",
        "            logits = self.predict(images, training=False)\n",
        "            # Select the class with the highest probability\n",
        "            \n",
        "            logits = tf.nn.softmax(logits)\n",
        "            logits = tf.reshape(logits, [-1, 10])\n",
        "            targets = tf.reshape(targets, [-1,10])\n",
        "            preds = tf.argmax(logits, axis=1)\n",
        "            goal = tf.argmax(targets, axis=1)\n",
        "            self.acc.update_state(goal, preds)\n",
        "            # Compute the accuracy\n",
        "            result = self.acc.result()\n",
        "            #result = self.acc.result().numpy()\n",
        "        return result\n",
        "\n",
        "\n",
        "    def fit_fc(self, training_data, eval_data, optimizer, num_epochs=500, \n",
        "            early_stopping_rounds=10, verbose=10, train_from_scratch=False, ckpoint=None):\n",
        "        \"\"\" Function to train the model, using the selected optimizer and\n",
        "            for the desired number of epochs. You can either train from scratch\n",
        "            or load the latest model trained. Early stopping is used in order to\n",
        "            mitigate the risk of overfitting the network.\n",
        "            \n",
        "            Args:\n",
        "                training_data: the data you would like to train the model on.\n",
        "                                Must be in the tf.data.Dataset format.\n",
        "                eval_data: the data you would like to evaluate the model on.\n",
        "                            Must be in the tf.data.Dataset format.\n",
        "                optimizer: the optimizer used during training.\n",
        "                num_epochs: the maximum number of iterations you would like to \n",
        "                            train the model.\n",
        "                early_stopping_rounds: stop training if the loss on the eval \n",
        "                                       dataset does not decrease after n epochs.\n",
        "                verbose: int. Specify how often to print the loss value of the network.\n",
        "                train_from_scratch: boolean. Whether to initialize variables of the\n",
        "                                    the last trained model or initialize them\n",
        "                                    randomly.\n",
        "        \"\"\" \n",
        "    \n",
        "        if train_from_scratch==False:\n",
        "            self.restore_model()\n",
        "        \n",
        "        # Initialize best loss. This variable will store the lowest loss on the\n",
        "        # eval dataset.\n",
        "        best_loss = 999.99\n",
        "        \n",
        "        # Initialize classes to update the mean loss of train and eval\n",
        "        train_loss = tf.keras.metrics.Mean('train_loss')\n",
        "        eval_loss = tf.keras.metrics.Mean('eval_loss')\n",
        "        acc_train = tf.keras.metrics.Mean('train_acc')\n",
        "        acc_val = tf.keras.metrics.Mean('val_acc')\n",
        "        \n",
        "        # Initialize dictionary to store the loss history\n",
        "        self.history = {}\n",
        "        self.history['train_loss'] = []\n",
        "        self.history['eval_loss'] = []\n",
        "        self.history['train_acc'] = []\n",
        "        self.history['val_acc'] = []\n",
        "        \n",
        "        # Begin training\n",
        "        with tf.device(self.device):\n",
        "            for i in range(num_epochs):\n",
        "                # Training with gradient descent\n",
        "                #training_data_x = training_data.shuffle(buffer_size=1024).batch(128)\n",
        "                for step, (images, target) in enumerate(training_data):\n",
        "                    grads = self.grads_fn(images, target, True)\n",
        "                    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "                    \n",
        "                # Compute the loss on the training data after one epoch\n",
        "                for step, (images, target) in enumerate(training_data):\n",
        "                    loss = self.loss_fn(images, target, False)\n",
        "                    accuracy = self.compute_accuracy_2(images,target)\n",
        "                    acc_train(accuracy)\n",
        "                    train_loss(loss)\n",
        "                self.history['train_loss'].append(train_loss.result())\n",
        "                self.history['train_acc'].append(acc_train.result())\n",
        "                #self.history['train_loss'].append(train_loss.result().numpy())\n",
        "                #self.history['train_acc'].append(acc_train.result().numpy())\n",
        "                # Reset metrics\n",
        "                \n",
        "                \n",
        "                # Compute the loss on the eval data after one epoch\n",
        "                for step, (images, target) in enumerate(eval_data):\n",
        "                    loss = self.loss_fn(images, target, False)\n",
        "                    accuracy = self.compute_accuracy_2(images,target)\n",
        "                    acc_val(accuracy)\n",
        "                    eval_loss(loss)\n",
        "                self.history['eval_loss'].append(eval_loss.result())\n",
        "                self.history['val_acc'].append(acc_val.result())\n",
        "                # self.history['eval_loss'].append(eval_loss.result().numpy())\n",
        "                # self.history['val_acc'].append(acc_val.result().numpy())\n",
        "                # Reset metrics\n",
        "                # print(train_loss.result())\n",
        "                # print('Train loss at epoch %d: ' %(i+1), train_loss.result())\n",
        "                # print('Train Acc at epoch %d: ' %(i+1), acc_train.result())\n",
        "                    \n",
        "                # print('Eval loss at epoch %d: ' %(i+1), eval_loss.result())\n",
        "                # print('Eval Acc at epoch %d: ' %(i+1), acc_val.result())\n",
        "                \n",
        "                train_loss.reset_states()\n",
        "                acc_train.reset_states()\n",
        "                eval_loss.reset_states()\n",
        "                acc_val.reset_states()\n",
        "                #Print train and eval losses\n",
        "                if (i==0) | ((i+1)%verbose==0):\n",
        "                    print('Train loss at epoch %d: ' %(i+1), self.history['train_loss'][-1])\n",
        "                    print('Train Acc at epoch %d: ' %(i+1), self.history['train_acc'][-1])\n",
        "                    \n",
        "                    print('Eval loss at epoch %d: ' %(i+1), self.history['eval_loss'][-1])\n",
        "                    print('Eval Acc at epoch %d: ' %(i+1), self.history['val_acc'][-1])\n",
        "                    print('Model Saved')\n",
        "                    ckpoint.save(file_prefix=self.checkpoint_prefix)\n",
        "                    #self.save_model()\n",
        "\n",
        "                # Check for early stopping\n",
        "                count = 0 \n",
        "                if self.history['eval_loss'][-1]<best_loss:\n",
        "                    best_loss = self.history['eval_loss'][-1]\n",
        "                    count = early_stopping_rounds\n",
        "                else:\n",
        "                    count -= 1\n",
        "                if count==0:\n",
        "                    break\n",
        "\n",
        "\n",
        "    def predict_fc(self, test_data):\n",
        "      # Initialize classes to update the mean loss of train and eval\n",
        "        test_loss = tf.keras.metrics.Mean('train_loss')\n",
        "        acc_test = tf.keras.metrics.Mean('test_acc')\n",
        "        \n",
        "        \n",
        "        # Begin training\n",
        "        with tf.device(self.device):\n",
        "          for step, (images, target) in enumerate(test_data):\n",
        "              loss = self.loss_fn(images, target, False)\n",
        "              accuracy = self.compute_accuracy_2(images,target)\n",
        "              acc_test(accuracy)\n",
        "              test_loss(loss)\n",
        "        print(\"test_accuracy %d\", acc_test.result())\n",
        "        print(\"test_loss %d\", test_loss.result())\n",
        "        # avg_mse = np.sum(test_loss.numpy()) / x_test.shape[0]\n",
        "        # print('Test Standard error: {:.4f}'.format(np.sqrt(avg_mse)))  \n",
        "                \n",
        "                \n",
        "\n"
      ],
      "metadata": {
        "id": "KGjSk_lMhb7V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotation_range = 15\n",
        "horizontal_flip = True\n",
        "\n",
        "# Define the custom preprocessing function to crop and resize the images\n",
        "def crop_resize(image):\n",
        "    h, w = image.shape[:2]\n",
        "    crop_size = min(h, w)\n",
        "    start_x = int((w - crop_size) / 2)\n",
        "    start_y = int((h - crop_size) / 2)\n",
        "    cropped_image = image[start_y:start_y+crop_size, start_x:start_x+crop_size]\n",
        "    resized_image = tf.image.resize(cropped_image, (224, 224))\n",
        "    return resized_image\n",
        "\n",
        "# Define the data generator for the training set\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=rotation_range,\n",
        "    horizontal_flip=horizontal_flip,\n",
        "    preprocessing_function=crop_resize\n",
        ")\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = train_datagen.flow_from_directory(\n",
        "    x_train,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "id": "d0tVNw0bngni",
        "outputId": "2594fbfe-799a-4377-d03d-bff190585e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-86cba778f2e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m train_dataset = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \"\"\"\n\u001b[0;32m-> 1648\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: listdir: embedded null character in path"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Specify the path where you want to save/restore the trained variables.\n",
        "#sample_data\n",
        "checkpoint_directory = '/content/models_checkpoints/'\n",
        "#checkpoint_directory = \"/tmp/training_checkpoints\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
        "# Use the GPU if available.\n",
        "device = 'gpu:0'\n",
        "\n",
        "# Define optimizer.\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)\n",
        "\n",
        "# Instantiate model. This doesn't initialize the variables yet.\n",
        "model = ImageRecognitionCNN(num_classes=10, device=device, \n",
        "                              checkpoint_directory=checkpoint_directory, checkpoint_prefix = checkpoint_prefix)\n",
        "\n",
        "\n",
        "### This example is modified to work with Tensorflow > 2.6\n",
        "\n",
        "## Create checkpoint to save your model\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
        "## Now store the initial checkpoint - at random initialization\n",
        "checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "metadata": {
        "id": "4a-iuiHIypry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "aff040bf-b4a4-4837-bd69-887718e2d1c5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/models_checkpoints/ckpt-1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "# Pass the saver object to save model each epoch -- You can change the rule/scheme for saving the model\n",
        "model.fit_fc(train_dataset, val_dataset, optimizer, num_epochs=20, \n",
        "          early_stopping_rounds=2, verbose=2, train_from_scratch=True, ckpoint = checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exUkL_OBytBw",
        "outputId": "aca05b57-4cd1-4429-e144-92fb7cc7bf89"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss at epoch 1:  tf.Tensor(1.3702708, shape=(), dtype=float32)\n",
            "Train Acc at epoch 1:  tf.Tensor(0.4962858, shape=(), dtype=float32)\n",
            "Eval loss at epoch 1:  tf.Tensor(1.3683777, shape=(), dtype=float32)\n",
            "Eval Acc at epoch 1:  tf.Tensor(0.49791503, shape=(), dtype=float32)\n",
            "Model Saved\n",
            "Train loss at epoch 2:  tf.Tensor(1.0577381, shape=(), dtype=float32)\n",
            "Train Acc at epoch 2:  tf.Tensor(0.53088146, shape=(), dtype=float32)\n",
            "Eval loss at epoch 2:  tf.Tensor(1.0754969, shape=(), dtype=float32)\n",
            "Eval Acc at epoch 2:  tf.Tensor(0.5570759, shape=(), dtype=float32)\n",
            "Model Saved\n",
            "Train loss at epoch 4:  tf.Tensor(0.76200217, shape=(), dtype=float32)\n",
            "Train Acc at epoch 4:  tf.Tensor(0.61808914, shape=(), dtype=float32)\n",
            "Eval loss at epoch 4:  tf.Tensor(0.840012, shape=(), dtype=float32)\n",
            "Eval Acc at epoch 4:  tf.Tensor(0.63334185, shape=(), dtype=float32)\n",
            "Model Saved\n",
            "Train loss at epoch 6:  tf.Tensor(0.6407755, shape=(), dtype=float32)\n",
            "Train Acc at epoch 6:  tf.Tensor(0.6690758, shape=(), dtype=float32)\n",
            "Eval loss at epoch 6:  tf.Tensor(0.81896853, shape=(), dtype=float32)\n",
            "Eval Acc at epoch 6:  tf.Tensor(0.67838705, shape=(), dtype=float32)\n",
            "Model Saved\n",
            "Train loss at epoch 8:  tf.Tensor(0.5452794, shape=(), dtype=float32)\n",
            "Train Acc at epoch 8:  tf.Tensor(0.6998744, shape=(), dtype=float32)\n",
            "Eval loss at epoch 8:  tf.Tensor(0.8318697, shape=(), dtype=float32)\n",
            "Eval Acc at epoch 8:  tf.Tensor(0.7068974, shape=(), dtype=float32)\n",
            "Model Saved\n",
            "Train loss at epoch 10:  tf.Tensor(0.38264495, shape=(), dtype=float32)\n",
            "Train Acc at epoch 10:  tf.Tensor(0.727118, shape=(), dtype=float32)\n",
            "Eval loss at epoch 10:  tf.Tensor(0.8215915, shape=(), dtype=float32)\n",
            "Eval Acc at epoch 10:  tf.Tensor(0.73387206, shape=(), dtype=float32)\n",
            "Model Saved\n",
            "Train loss at epoch 12:  tf.Tensor(0.27832907, shape=(), dtype=float32)\n",
            "Train Acc at epoch 12:  tf.Tensor(0.75081193, shape=(), dtype=float32)\n",
            "Eval loss at epoch 12:  tf.Tensor(0.8600326, shape=(), dtype=float32)\n",
            "Eval Acc at epoch 12:  tf.Tensor(0.7570191, shape=(), dtype=float32)\n",
            "Model Saved\n",
            "Train loss at epoch 14:  tf.Tensor(0.2589302, shape=(), dtype=float32)\n",
            "Train Acc at epoch 14:  tf.Tensor(0.77087206, shape=(), dtype=float32)\n",
            "Eval loss at epoch 14:  tf.Tensor(1.0453985, shape=(), dtype=float32)\n",
            "Eval Acc at epoch 14:  tf.Tensor(0.7755114, shape=(), dtype=float32)\n",
            "Model Saved\n",
            "Train loss at epoch 16:  tf.Tensor(0.25775778, shape=(), dtype=float32)\n",
            "Train Acc at epoch 16:  tf.Tensor(0.78754634, shape=(), dtype=float32)\n",
            "Eval loss at epoch 16:  tf.Tensor(1.1877147, shape=(), dtype=float32)\n",
            "Eval Acc at epoch 16:  tf.Tensor(0.79110914, shape=(), dtype=float32)\n",
            "Model Saved\n",
            "Train loss at epoch 18:  tf.Tensor(0.19792043, shape=(), dtype=float32)\n",
            "Train Acc at epoch 18:  tf.Tensor(0.80126894, shape=(), dtype=float32)\n",
            "Eval loss at epoch 18:  tf.Tensor(1.2766706, shape=(), dtype=float32)\n",
            "Eval Acc at epoch 18:  tf.Tensor(0.8045524, shape=(), dtype=float32)\n",
            "Model Saved\n",
            "Train loss at epoch 20:  tf.Tensor(0.17594694, shape=(), dtype=float32)\n",
            "Train Acc at epoch 20:  tf.Tensor(0.81332666, shape=(), dtype=float32)\n",
            "Eval loss at epoch 20:  tf.Tensor(1.3614683, shape=(), dtype=float32)\n",
            "Eval Acc at epoch 20:  tf.Tensor(0.81618977, shape=(), dtype=float32)\n",
            "Model Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict_fc(test_dataset)"
      ],
      "metadata": {
        "id": "0Z13NUdHlqeU",
        "outputId": "370b5b62-7641-4eb1-c237-96dd8869766e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_accuracy %d tf.Tensor(0.8155646, shape=(), dtype=float32)\n",
            "test_loss %d tf.Tensor(1.4327943, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's try to restore the model from latest checkpoint\n",
        "# In this case load latest checkpoint -- 7\n",
        "checkpoint_prefix_restore = os.path.join(checkpoint_directory, \"ckpt-7\")\n",
        "checkpoint.restore(checkpoint_prefix_restore)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQMGyBPnavyK",
        "outputId": "1fa7bd20-54db-4a59-af39-d9029abaa8d1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f4d1f147160>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the model and make prediction\n",
        "c_model = checkpoint.model\n",
        "c_model.predict_fc(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wRI4vAreD70",
        "outputId": "134e7cf7-a7de-4671-c8b2-3de248a5b2c4"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_accuracy %d tf.Tensor(0.7339465, shape=(), dtype=float32)\n",
            "test_loss %d tf.Tensor(0.85151184, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+## Now let's define various tasks\n",
        "\n",
        "Given we have provided entire training scripts and code that shows how to run a Convolution neural network on cifar-10 datasets. For this assignment the class will work on 3 tasks,\n",
        "\n",
        "## Very important - If you start late, you won't be able to finish this assignment. \n",
        "\n",
        "## Task-1\n",
        "1. Find the best hyper-parameter for above model --- You have to add minimum one additional conv layer to above model -- you can reduce the filter_size and input features. But not the number of layers\n",
        "2. It is very important you Keep track of your validation loss to select the best model -- You will train your model for minimum 15 epochs.\n",
        "3. There is flexibility to change your network hyperparameters, such as optimizer, learning rate, activation function, filter size, batch size, etc\n",
        "4. Show minimum 3 settings or comparsion.\n",
        "5. Once you have the best hyper-parameter, then run your model for 5 trials, report mean accuracy and standard error (both metrics are calculated over your accuracy).\n",
        "6. Whenever in doubt, simply ask.\n",
        "\n",
        "## Task-2 - Data Augmentation\n",
        "1. Rotate Image at an arbitrary angle (15,45, 60 and so on)\n",
        "2. Crop Image from the center and resize\n",
        "3. Flip Image from left to right (Create a mirror image)\n",
        "4. Choose any two augmentation approach and report your performance. \n",
        "5. Remember you will optimize your model, try to beat accuracy reported on task-1. Once you find the best settings run your experiments for 5 trials, report mean accuracy and standard error (both metrics are calculated over your accuracy)\n",
        "6. You will only augment your train set, test and validation set are untouched. \n",
        "\n",
        "## Task-3- Adversial Samples\n",
        "1. Now you will add small gaussian noise to your test samples(one can use standard deviation = 0.05, while using random_normal)\n",
        "2. Take model from task-1, get the performance on this adversial samples. For this One must report confusion matrix on original test samples and adversial samples. (The code is already provided in Github or else one can use sklearn function). Remember your confusion matrix is the product of 5 trials, in other words you will pass mean values or mean prediction to your function\n",
        "3. Repeat above steps for task-2\n",
        "4. Report your findings."
      ],
      "metadata": {
        "id": "go8mOBRNnIPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rubric\n",
        "\n",
        "## Task-1 (3 points)\n",
        "1. Only one setting is reported - (-2 point)\n",
        "2. No modification to original code and only reported default settings - (0 or -3 points)\n",
        "3. Mean and standard error miscalculations/error - (1.2 points)\n",
        "4. Result with only one trial - (-2 points)\n",
        "\n",
        "## Task-2 (4 points)\n",
        "1. No data augmentation - (- 4 points)\n",
        "2. Only one data augmentation - (- 2 points)\n",
        "3. Data augmentation on test and validation set - (0 or -4 points)\n",
        "4. Mean and standard error miscalculations/error - (1.2 points)\n",
        "5. Result with only one trial - (-2.8 points)\n",
        "\n",
        "## Task-3 (3 points)\n",
        "1. gaussian noise added on test and validation set - (0 or -3 points)\n",
        "2. Result with only one trial - (-2 points)\n",
        "3. Performance reported on only one task (1 or 2) - (-1.5 points)\n",
        "4. No confusion matrix - (- 1.2 points)\n",
        "5. Mean and standard error miscalculations/error - (1.2 points)"
      ],
      "metadata": {
        "id": "6KPxdyZOq_IG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Po6TSVvzwXC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}