{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavankumarm2505/Intro_to_DL/blob/main/Copy_of_Assignment_1_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment - 0001\n",
        "1. Build your first Neural Network and test it on MNIST and F-MNIST/K-MNIST\n",
        "2. No - Plagarism. If solutions match, then the student who shared the solution and students who copied will be reported to USF and will get F in this course.\n",
        "3. You can discuss, but have to code and provide your own solution. Provide name of the person or persons with whom you had discussion"
      ],
      "metadata": {
        "id": "oC1UJhcn6cun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Things to Do\n",
        " \n",
        "1. Create a Neural Network with 3 layers, (W1, W2, W3)\n",
        "2. H1 = $X \\cdot W1$ + b1, Z1 = $\\sigma$(H1), where $\\sigma$ is your activation function, $\\cdot$ is your hardamard product, H denotes pre-activation and Z denotes post-activation.\n",
        "3. Recursively construct your NN, H2 = (Z1 $\\cdot$W2) + b2, Z2 = $\\sigma$(H2) and final output layer is H3 = (Z2 $\\cdot$ W3) + b3, z3 = $σ^1$(H3), where $σ^1$ is your softmax function\n",
        "4. Normalize your data within 0-1 range, create one-hot encoding for your labels.\n",
        "5. Take 3digit of your first name, and convert them based on character indices - For example: Ank ---> 1 14 11, so my seed is 11411. This will ensure each students have unique seeds.\n",
        "6. Run your experiments for minimum $10$ epochs. Remember one epoch is entire pass through data.\n",
        "7. You will run each experiment min $5$ times and report your average accuracy and standard error -- Change seed per trial\n",
        "8. We have provided 2 folders, members of Teams A - E will work on data that says Team_red and Team F- K will work on data that says Team_blue\n",
        "9. Important: All parameter optimization should be done on Validation/Dev Set, you will only do one pass inference over test set.\n",
        "10. Complete all code blocks highlighted as TODO\n",
        "11. Sumbit your code, your report with findings and upload on canvas."
      ],
      "metadata": {
        "id": "tRw25rAH7Owu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rubric\n",
        "1. Total marks = 10% of your total grade. \n",
        "2. This assignment is of worth 100 points, which will be converted into 10.\n",
        "3. If you train on Vaidation and test sets ---- (0 or -100 Points)\n",
        "4. Results with only one trial is reported ---- (-20 points)\n",
        "5. Experiments are performed on only one dataset ---- (-50 points)\n",
        "6. Code is not working or has some bugs, Depending on bug ---- (-15 to -60 points)\n",
        "7. Submiting some random solution without following template ---- (-100 points)"
      ],
      "metadata": {
        "id": "OQPpqCXdHPSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "\n",
        "seed = 16122\n",
        "np.random.seed(seed) ## Use your own unique seed\n",
        "tf.random.set_seed(seed) ## Use your own unique seed\n",
        "\n",
        "## Authors - Ankur Mali\n",
        "## Code designed for intro to DL at University of South Florida"
      ],
      "metadata": {
        "id": "EJJE2RGZL8qo"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define model specific parameters\n",
        "## Repalce None with values\n",
        "size_input = 28*28\n",
        "size_hidden = 256\n",
        "size_hidden1 = 256\n",
        "size_hidden2 = 256\n",
        "size_hidden3 = 256\n",
        "size_output = 10\n",
        "number_of_train_examples = 50000\n",
        "number_of_dev_examples = 10000\n",
        "number_of_test_examples = 10000\n",
        "batch_size = 32\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "D32LRkMJGV2V"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO\n",
        "## Load Data\n",
        "X_train = np.load('X_train.npy').reshape(-1,784)\n",
        "print(X_train.shape)\n",
        "X_test = np.load('X_test.npy').reshape(-1,784)\n",
        "## Create Validation data from train splits\n",
        "\n",
        "Y_train = np.load('y_train.npy')\n",
        "Y_test = np.load('y_test.npy')\n",
        "X_val, Y_val = X_train[50000:], Y_train[50000:] \n",
        "X_train,Y_train = X_train[:50000], Y_train[:50000]\n",
        "\n"
      ],
      "metadata": {
        "id": "3IVzVDIR6arH",
        "outputId": "648838f4-d927-40a0-d5d9-9fe15171cb8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO\n",
        "# Normalize your X\n",
        "X_train = (X_train - np.mean(X_train,axis=0)) / np.std(X_train,axis=0)\n",
        "X_val = (X_val - np.mean(X_val,axis=0)) / np.std(X_val,axis=0)\n",
        "X_test = (X_test - np.mean(X_test,axis=0)) / np.std(X_test,axis=0)\n",
        "\n",
        "# Convert your labels into one-hot encoding\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, size_output)\n",
        "Y_val = tf.keras.utils.to_categorical(Y_val,size_output)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test,size_output)\n",
        "# Hint look into function tf.keras.utils.to_categorical or Other function is tf.one_hot()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qSrIHwVxRbYc"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    ### TODO\n",
        "    ### Declare all your variables\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output]))\n",
        "\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    ## TODO\n",
        "\n",
        "    # Convert your pred and true to tf.float32\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "\n",
        "    # Ensure your shapes are (batch_size, size_output)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    # This above function will add softmax to your final layer and then perform cross-entropy\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    \n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    lr = 0.1 #Play with your learning rate\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = lr) # For this excercise we will only use SGD\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables)) # Optimizer\n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    ## TODO\n",
        "    H1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    Z1 = tf.nn.sigmoid(H1)\n",
        "    H2 = tf.matmul(Z1, self.W2) + self.b2\n",
        "    Z2 = tf.nn.sigmoid(H2)\n",
        "    H3 = tf.matmul(Z2, self.W3) + self.b3\n",
        "    \n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    \n",
        "\n",
        "    # Compute output\n",
        "    output = H3 # Logits\n",
        "    \n",
        "    # Remember the loss function has keras loss objective, that explicitly applies softmax over logits\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "ycucHIu9OWWi"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(25, seed=epoch*(seed)).batch(batch_size)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  accuracy_v = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_gpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  \n",
        "  ## TODO\n",
        "\n",
        "  ## Now calculate Validation Accuracy\n",
        "\n",
        "  preds = mlp_on_gpu.forward(X_val)\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_val, 1))\n",
        "  accuracy_v =  tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy_v.numpy()\n",
        "\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "id": "vDOs-cNLQbPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacd5220-49a3-4f04-eadb-6afdd86a67c9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.7747\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.0496722314453125 \n",
            "\n",
            "Validation Accuracy: 0.7384\n",
            "\n",
            "Train Accuracy: 0.8150\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.02460473388671875 \n",
            "\n",
            "Validation Accuracy: 0.7630\n",
            "\n",
            "Train Accuracy: 0.8350\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.018439703369140626 \n",
            "\n",
            "Validation Accuracy: 0.7756\n",
            "\n",
            "Train Accuracy: 0.8434\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.015484481201171876 \n",
            "\n",
            "Validation Accuracy: 0.7699\n",
            "\n",
            "Train Accuracy: 0.8601\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.013615498046875 \n",
            "\n",
            "Validation Accuracy: 0.7796\n",
            "\n",
            "Train Accuracy: 0.8699\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.012320548095703125 \n",
            "\n",
            "Validation Accuracy: 0.7834\n",
            "\n",
            "Train Accuracy: 0.8669\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.011348411865234375 \n",
            "\n",
            "Validation Accuracy: 0.7726\n",
            "\n",
            "Train Accuracy: 0.8901\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.010570909423828125 \n",
            "\n",
            "Validation Accuracy: 0.7879\n",
            "\n",
            "Train Accuracy: 0.8990\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.009855189208984375 \n",
            "\n",
            "Validation Accuracy: 0.7933\n",
            "\n",
            "Train Accuracy: 0.8822\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.009271611328125 \n",
            "\n",
            "Validation Accuracy: 0.7770\n",
            "\n",
            "Total time taken (in seconds): 352.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO - Write a inference code and perform only one pass through test data\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(4)\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_gpu.forward(inputs)\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "\n",
        "print('Test MSE: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n"
      ],
      "metadata": {
        "id": "jpZC6lpCRSe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d4ea49-4279-4c8e-da91-f7eda3a92cda"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 0.4244\n"
          ]
        }
      ]
    }
  ]
}